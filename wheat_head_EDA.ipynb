{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# import data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show, output_file\n",
    "from bokeh.models import ColumnDataSource, HoverTool, Panel\n",
    "from bokeh.models.widgets import Tabs\n",
    "\n",
    "# import data augmentation\n",
    "import albumentations as A\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Image\n",
    "\"\"\"\n",
    "train_dir = '../input/global-wheat-detection/train/'\n",
    "test_dir = '../input/global-wheat-detection/test/'\n",
    "\n",
    "train_path = glob(train_dir + '/*')\n",
    "test_path = glob(test_dir + '/*')\n",
    "\n",
    "print('Number of train path: ', len(train_path))\n",
    "print('Number of test path: ', len(test_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We only have 10 test images and those are very low.\n",
    "* we have 3422 train images which are quite small for training a deep neural network model so data augmentation and transfer learning techniques will sure come in handy a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Annotations\n",
    "\"\"\"\n",
    "train_df = pd.read_csv('../input/global-wheat-detection/train.csv')\n",
    "print(train_df.head(5))\n",
    "print('Number of rows are: ', train_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Each row cosists of an image id, shape of image,Bbox and source details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Shape check\n",
    "\"\"\"\n",
    "print('Shapes of width: ', train_df['width'].unique().item())\n",
    "print('Shapes of height: ', train_df['height'].unique().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ All train images have shape 1024x1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a dataframe with splitted boxes.\n",
    "\"\"\"\n",
    "all_train_images = pd.DataFrame([i.split('/')[-1][:-4] for i in train_path])\n",
    "all_train_images.columns = ['image_id']\n",
    "\n",
    "# Merge with the rest columns\n",
    "all_train_images = all_train_images.merge(train_df, on='image_id', how = 'left')\n",
    "\n",
    "# Fill nan boxes with zeros \n",
    "all_train_images['bbox'] = all_train_images.bbox.fillna('[0,0,0,0]')\n",
    "\n",
    "# Split bbox infos to 4 cols\n",
    "# Create the sub-dataframe which has splitted box infos\n",
    "bbox_items = all_train_images.bbox.str.split(',', expand = True)\n",
    "\n",
    "all_train_images['x_min'] = bbox_items[0].str.strip('[ ').astype(float)\n",
    "all_train_images['y_min'] = bbox_items[1].str.strip(' ').astype(float)\n",
    "all_train_images['width'] = bbox_items[2].str.strip(' ').astype(float)\n",
    "all_train_images['height'] = bbox_items[3].str.strip('] ').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check images with no bboxes.\n",
    "\"\"\"\n",
    "# Removes img w/no bboxes in all_train_images dataframe\n",
    "all_train_images = all_train_images[all_train_images.width!=None]\n",
    "\n",
    "# Compare processed dataframe to init dataframe\n",
    "print('There are {} images with no bounding box!'.format(len(all_train_images)- len(train_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used for connecting boxes with id. \n",
    "def get_all_bboxes(df, image_id):\n",
    "    image_bboxes = df[df.image_id == image_id]   \n",
    "    bboxes = []\n",
    "    for _,row in image_bboxes.iterrows():\n",
    "        bboxes.append((row.x_min, row.y_min, row.width, row.height))\n",
    "        \n",
    "    return bboxes\n",
    "\n",
    "def plot_image_examples(df, rows=3, cols=3):\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(25,20))\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            idx = np.random.randint(len(df), size=1)[0]\n",
    "            img_id = df.iloc[idx].image_id\n",
    "       \n",
    "            img = Image.open(train_dir + img_id + '.jpg')\n",
    "            axs[row, col].imshow(img)\n",
    "            \n",
    "            bboxes = get_all_bboxes(df, img_id)\n",
    "            \n",
    "            for bbox in bboxes:\n",
    "                rect = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none')\n",
    "                axs[row, col].add_patch(rect)\n",
    "            \n",
    "            axs[row, col].axis('off')\n",
    "            \n",
    "            \n",
    "# Sample images with bounding box.\n",
    "plot_image_examples(all_train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bboxes analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_hover(dataframe, column, colors=[\"#94c8d8\", \"#ea5e51\"], bins=30, title=''):\n",
    "    \"\"\"Count the appearance of values in one column and plot the histogram.\n",
    "    \"\"\"\n",
    "    hist, edges = np.histogram(dataframe[column], bins = bins)\n",
    "    \n",
    "    hist_df = pd.DataFrame({column: hist,\n",
    "                             \"left\": edges[:-1],\n",
    "                             \"right\": edges[1:]})\n",
    "    hist_df[\"interval\"] = [\"%d to %d\" % (left, right) for left, \n",
    "                           right in zip(hist_df[\"left\"], hist_df[\"right\"])]\n",
    "\n",
    "    src = ColumnDataSource(hist_df)\n",
    "    plot = figure(plot_height = 400, plot_width = 600,\n",
    "          title = title,\n",
    "          x_axis_label = column,\n",
    "          y_axis_label = \"Count\")    \n",
    "    plot.quad(bottom = 0, top = column,left = \"left\", \n",
    "        right = \"right\", source = src, fill_color = colors[0], \n",
    "        line_color = \"#35838d\", fill_alpha = 0.7,\n",
    "        hover_fill_alpha = 0.7, hover_fill_color = colors[1])\n",
    "        \n",
    "    hover = HoverTool(tooltips = [('Interval', '@interval'),\n",
    "                              ('Count', str(\"@\" + column))])\n",
    "    plot.add_tools(hover)\n",
    "    \n",
    "    output_notebook()\n",
    "    show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area_bbox(bbox):\n",
    "    bbox = literal_eval(bbox)\n",
    "    return bbox[2]*bbox[3]\n",
    "\n",
    "all_train_images['area_bboxes'] = all_train_images['bbox'].apply(get_area_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_hover(all_train_images, 'area_bboxes', title='Bboxes area distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimum area of bboxes: ', all_train_images.area_bboxes.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimum area of bboxes (>0): ', all_train_images[all_train_images['area_bboxes'] > 0].min()['area_bboxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As organizers say, there are many bounding boxes for each image, and not all images include wheat heads / bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bboxes analysis by area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "\n",
    "\"\"\"Histogram of number of bboxes per image.\n",
    "\"\"\"\n",
    "fig.set_size_inches(16,5)\n",
    "ax1.hist(all_train_images['image_id'].value_counts(), bins=30)\n",
    "ax1.set_title('Number of spikes per image')\n",
    "ax1.set_xlabel('Bounding boxes')\n",
    "ax1.set_ylabel('Images')\n",
    "\n",
    "\n",
    "ax2.set_xlim(100, 125)\n",
    "ax2.set_ylim(0,10)\n",
    "ax2.hist(all_train_images['image_id'].value_counts(), bins=30)\n",
    "ax2.set_title('Zooming left part of histogram')\n",
    "ax2.set_xlabel('Bounding boxes')\n",
    "ax2.set_ylabel('Images')\n",
    "ax2.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Max number of bounding boxes is 116, whereas min (annotated) number is 1.\n",
    "+ Most of the images have 20-50 wheat heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total bboxes area per img\n",
    "area_per_image = all_train_images.groupby(by='image_id').sum().reset_index()\n",
    "\n",
    "\"\"\"area_per_image_precent: for each image, calculate bboxes area/image area.\n",
    "\"\"\"\n",
    "area_per_image_precent = area_per_image.copy()\n",
    "area_per_image_precent['area_bboxes'] = area_per_image_precent['area_bboxes']/(1024*1024)\n",
    "\n",
    "hist_hover(area_per_image_precent, 'area_bboxes', title='Percentage of image area covered by bounding boxes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ This looks like a nice normal distribution! 20-40% of the image area is covered by the bounding boxes.\n",
    "+ This observation can be used to validate the predictions of the resulting model. The percentage of predicted bounding boxes area should be normally distributed too.\n",
    "+ We can also see that the maximum is actually greater than 100%. This means that the bounding boxes are overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Image with small covered bboxes.\n",
    "\"\"\"\n",
    "small_boxes_ids = all_train_images[(all_train_images['area_bboxes'] < 50) & (all_train_images['area_bboxes'] > 0)].image_id\n",
    "\n",
    "plot_image_examples(all_train_images[all_train_images.image_id.isin(small_boxes_ids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you look very close, you can probably see those tinyest bounding boxes near the corners and borders of the images. Probably, the boundries were drawn first, than the images were cut into several ones. That is why we see those strange small bounsing boxes in the corners.  \n",
    "* It is not necessary to clean these, because they won't have much effect on the IOU metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_boxes_ids = all_train_images[all_train_images['area_bboxes'] > 200000].image_id\n",
    "\n",
    "plot_image_examples(all_train_images[all_train_images.image_id.isin(large_boxes_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_area_percent = area_per_image_precent[area_per_image_precent['area_bboxes'] < 7].image_id\n",
    "\n",
    "plot_image_examples(all_train_images[all_train_images.image_id.isin(small_area_percent)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bboxes analysis per source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_area = all_train_images.groupby('source', as_index = False)['area_bboxes'].mean()\n",
    "avg_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.bar(avg_area['source'],avg_area['area_bboxes'],align='center')\n",
    "plt.title('Bar plot for avg Bbox area feature')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('Avg Area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dict(all_train_images.source.value_counts())\n",
    "fig,ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(list(counts.values()), autopct='%1.1f%%',shadow=True, startangle=90);\n",
    "ax.legend(wedges, list(counts.keys()),\n",
    "          title=\"Sources\",\n",
    "          loc=\"center\",\n",
    "          bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "plt.setp(autotexts, size=15);\n",
    "\n",
    "ax.set_title(\"Data Distribution based on Sources\")\n",
    "plt.show()\n",
    "sns.countplot(x='source', data=all_train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Most of the images are produced by arvalis_1 source followed by ethz_1.\n",
    "+ I noticed that images came from different sources and each source contributed different amout of images for the training.\n",
    "+ Total 7 sources out of which arvalis_2 and inrae_1 are minority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_images['source'] = all_train_images['source'].fillna('No source')\n",
    "all_train_images.source.unique()\n",
    "\n",
    "print('No of images with no bounding boxes are',all_train_images[all_train_images['source']=='No source'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Images w/o source is image w/o bboxes.\n",
    "\"\"\"\n",
    "plot_image_examples(all_train_images[all_train_images.source == 'No source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_examples(all_train_images[all_train_images['source'] == 'ethz_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Density of wheat heads are more in this source of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_examples(all_train_images[all_train_images['source'] == 'arvalis_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Images have very low bounding boxes to high bounding boxes count.\n",
    "+ Some images have a lot of background data compared to wheat heads.\n",
    "+ Images might be taken in different light conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_examples(all_train_images[all_train_images['source'] == 'usask_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Images have very low bounding boxes to high bounding boxes count.\n",
    "+ Some images have a lot of background data compared to wheat heads.\n",
    "+ Images might be taken in different light conditions.\n",
    "+ Images contain mostly leaves instead of wheat heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_examples(all_train_images[all_train_images.source == 'rres_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Color of the leaves showcase yellow tint rather than green.\n",
    "* Avergae number of bounding boxes.\n",
    "* Mostly taken in dark lighting conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_examples(all_train_images[all_train_images.source == 'arvalis_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Images look so bright.\n",
    "* Most of the images are taken in bright day light conditions.\n",
    "* Images have average to more bounding boxes, so its good.\n",
    "* Most images have green and brown tint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_examples(all_train_images[all_train_images.source == 'arvalis_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Images are taken in brighter conditions.\n",
    "* Less wheat heads compared to other stuff in the images.\n",
    "* Wheat heads are almost same color as leaves...so it might be harder to detect a litle bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = pd.DataFrame(all_train_images.image_id.unique())\n",
    "images_df.columns = ['image_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_brightness(img):\n",
    "    img = cv2.imread(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return np.array(img).mean()\n",
    "\n",
    "def get_brightness(df):\n",
    "    brightness = [] \n",
    "    for _, row in df.iterrows():\n",
    "        img_id = row.image_id  \n",
    "        image_name = train_dir + img_id + '.jpg'\n",
    "        brightness.append(get_image_brightness(image_name))\n",
    "        \n",
    "    brightness_df = pd.DataFrame(brightness)\n",
    "    brightness_df.columns = ['brightness']\n",
    "    df = pd.concat([df, brightness_df], ignore_index = True, axis = 1)\n",
    "    df.columns = ['image_id', 'brightness']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brightness_df = get_brightness(images_df)\n",
    "all_train_images = all_train_images.merge(brightness_df, on = 'image_id')\n",
    "\n",
    "all_train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_hover(all_train_images, 'brightness', title='Images brightness distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bright_ids = all_train_images[all_train_images['brightness'] > 130]\n",
    "print('Number of bright images (greater than 170): {}'.format(len(bright_ids.image_id)), end='\\n-------\\n')\n",
    "print('Source statistics of images: \\n', bright_ids.source.value_counts(), end='\\n-------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot dark imgs.\n",
    "\"\"\"\n",
    "dark_ids = all_train_images[all_train_images['brightness'] < 30].image_id\n",
    "plot_image_examples(all_train_images[all_train_images.image_id.isin(dark_ids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On some of the images, it is even hard for human to see the spikes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot light imgs.\n",
    "\"\"\"\n",
    "bright_ids = all_train_images[all_train_images['brightness'] > 140].image_id\n",
    "plot_image_examples(all_train_images[all_train_images.image_id.isin(bright_ids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are very different from the dark ones. Some filters needed here to make the spikes more clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Green and yellow images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to plot images with different dominant colors. The idea is that the most green images will represent healthy plants. The most yellow images will contain plants close to maturity. The most brown images will have ground on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent_color_pix(image, mode):\n",
    "    \"\"\"Calculate percent of specific color in image.\n",
    "    \"\"\"\n",
    "    # convert to hsv\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    mask = np.zeros((image.shape[0],image.shape[1], 3), np.uint8)\n",
    "    \n",
    "    if mode == 'green': \n",
    "        # get the green mask\n",
    "        hsv_lower = (40, 40, 40) \n",
    "        hsv_higher = (70, 255, 255)\n",
    "        mask = cv2.inRange(hsv, hsv_lower, hsv_higher)\n",
    "        \n",
    "    elif mode == 'yellow': \n",
    "        # get the green mask\n",
    "        hsv_lower = (25, 40, 40) \n",
    "        hsv_higher = (35, 255, 255)\n",
    "        mask = cv2.inRange(hsv, hsv_lower, hsv_higher)  \n",
    "        \n",
    "    return (np.sum(mask)) / 255 / (1024 * 1024)\n",
    "\n",
    "def add_color_pixels_percentage(df, mode):\n",
    "    \"\"\"Add above result to df. \n",
    "    \"\"\"\n",
    "    color = []\n",
    "    for _, row in df.iterrows():\n",
    "        img_id = row.image_id  \n",
    "        image = cv2.imread(train_dir + img_id + '.jpg')\n",
    "        color.append(get_percent_color_pix(image, mode))\n",
    "        \n",
    "    color_df = pd.DataFrame(color)\n",
    "    \n",
    "    if mode == 'green':\n",
    "        color_df.columns = ['green_pixels']\n",
    "        df = pd.concat([df, color_df], ignore_index=True, axis=1)\n",
    "        df.columns = ['image_id', 'green_pixels']\n",
    "        \n",
    "    elif mode == 'yellow':\n",
    "        color_df.columns = ['yellow_pixels']\n",
    "        df = pd.concat([df, color_df], ignore_index=True, axis=1)\n",
    "        df.columns = ['image_id', 'yellow_pixels']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column with the percentage of green and yellow pixels\n",
    "green_pixels_df = add_color_pixels_percentage(images_df, mode ='green')\n",
    "yellow_pixels_df = add_color_pixels_percentage(images_df, mode ='yellow')\n",
    "\n",
    "all_train_images = all_train_images.merge(green_pixels_df, on='image_id')\n",
    "all_train_images = all_train_images.merge(yellow_pixels_df, on='image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Green pixels histogram.\n",
    "\"\"\"\n",
    "hist_hover(all_train_images, 'green_pixels', title='Percentage of green pixels distribution', colors=['#c3ea84', '#3e7a17'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Surprisingly, the most green images have only aoung 60% green pixels.\n",
    "\n",
    "+ The majority of the images are not green at all! Most probably, they are more yellow and are the images of the plants close to harvest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_ids = all_train_images[all_train_images['green_pixels'] > 0.6].image_id\n",
    "plot_image_examples(all_train_images[all_train_images.image_id.isin(green_ids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most green images mostly contain the plants with very small spikes, which are just starting to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_green_ids = all_train_images[all_train_images['green_pixels'] < 0.03].image_id\n",
    "plot_image_examples(all_train_images[all_train_images.image_id.isin(not_green_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Yellow pixels histogram.\n",
    "\"\"\"\n",
    "hist_hover(all_train_images, 'yellow_pixels', title='Percentage of yellow pixels distribution', colors=['#fffedb', '#fffeab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_ids = all_train_images[all_train_images['yellow_pixels'] > 0.6].image_id\n",
    "plot_image_examples(all_train_images[all_train_images.image_id.isin(yellow_ids)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from the above analysis\n",
    "* It is evident from the above image analysis that each type of source has specific types of wheat images. This is probably dependent on the region of data collection and time of the year when the particular data was collected.\n",
    "* There are some images with no bounding boxes at all.\n",
    "* Count of bounding boxes will vary from a min of 1 to max of 116.\n",
    "* Most of the images conatin an average of 20 to 60 bounding boxes.\n",
    "* Images given are taken from different lighting conditions at differen plant growth level.\n",
    "* Bounding boxes are messy!\n",
    "* Giant bounding boxes should be filtered out by area and removed before model training.\n",
    "* Micro bounding boxes. These can stay. They won't have much effect on the IOU metric.\n",
    "* Some spikes are not surrounded by a bounding box (missing bounding boxes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation\n",
    "Data augmentation is critical in this competition, because there is a relatively small training set. Data augmentation will allow to build robust models under given circumstances. What augmentations/filers might work:\n",
    "\n",
    "+ flipping images horizontally and vertically, because the orientation of original images is different;\n",
    "+ crop-resize, because we can see spikes at different zoom levels;\n",
    "+ different filters to adjust the lighting conditions. I suggest looking at this competition for example.\n",
    "What to do with caution:\n",
    "\n",
    "**rotation might not work, because rotation messes up the bounding boxes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup an example augmentation pipeline\n",
    "# be sure to use bbox safe functions for data augmentation\n",
    "transforms = A.Compose([\n",
    "    A.RandomSizedBBoxSafeCrop(512, 512, erosion_rate=0.0, interpolation=1, p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.OneOf([A.RandomContrast(),\n",
    "                A.RandomGamma(),\n",
    "                A.RandomBrightness()], p=1.0),\n",
    "    A.CLAHE(p=1.0)], p=1.0, bbox_params=A.BboxParams(format='coco', label_fields=['category_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transform(transforms, df, n_transforms=3):\n",
    "    \"\"\"Apply augmented transformation in random image.\n",
    "    \"\"\"\n",
    "    # Get random image by index\n",
    "    idx = np.random.randint(len(df), size=1)[0]\n",
    "    image_id = df.iloc[idx].image_id\n",
    "    \n",
    "    # Get bboxes info\n",
    "    bboxes = []\n",
    "    for _, row in df[df.image_id == image_id].iterrows():\n",
    "        bboxes.append([row.x_min, row.y_min, row.width, row.height])\n",
    "        \n",
    "    image_path = train_dir + image_id + '.jpg'\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, n_transforms+1, figsize=(15,7))\n",
    "    \n",
    "    # plot the original image\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_title('original')\n",
    "    for bbox in bboxes:\n",
    "        rect = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none')\n",
    "        axs[0].add_patch(rect)\n",
    "        \n",
    "    # apply transforms n_transforms times\n",
    "    for i in range(n_transforms):\n",
    "        params = {'image': np.asarray(image),\n",
    "                  'bboxes': bboxes,\n",
    "                  'category_id': [1 for j in range(len(bboxes))]}\n",
    "        \n",
    "        # Get transformation\n",
    "        augmented_boxes = transforms(**params)\n",
    "        \n",
    "        bboxes_aug = augmented_boxes['bboxes']\n",
    "        image_aug = augmented_boxes['image']\n",
    "        \n",
    "        # plot the augmented image and augmented bounding boxes\n",
    "        axs[i+1].imshow(image_aug)\n",
    "        axs[i+1].set_title('augmented_' + str(i+1))\n",
    "        for bbox in bboxes_aug:\n",
    "            rect = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none')\n",
    "            axs[i+1].add_patch(rect)\n",
    "            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_transform(transforms, all_train_images, n_transforms=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_transform(transforms, all_train_images, n_transforms=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_transform(transforms, all_train_images, n_transforms=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
